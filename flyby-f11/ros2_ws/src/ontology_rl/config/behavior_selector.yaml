# Behavior Selector RL Training Configuration
# Level 2: Tactical behavior selection (1s decision horizon)

# Environment settings
env_id: 'FlybyBehaviorSelector-v0'
env_kwargs:
  max_steps: 500

# Algorithm: PPO
algorithm: 'PPO'

# Training parameters
total_timesteps: 200000
n_envs: 1

# PPO hyperparameters
learning_rate: 0.0003
n_steps: 512
batch_size: 32
n_epochs: 10
gamma: 0.98
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.02
vf_coef: 0.5
max_grad_norm: 0.5

# Network architecture
policy_kwargs:
  net_arch:
    - 64
    - 64

# Reproducibility
seed: 42

# Logging
tensorboard_log: './logs/behavior_selector'
log_interval: 10
verbose: 1

# Checkpointing
save_freq: 20000
save_path: './models/behavior_selector'

# Evaluation
eval_freq: 10000
n_eval_episodes: 10

# Behavior primitives
behaviors:
  - name: hover
    id: 0
    description: "Hold position"
  - name: cruise
    id: 1
    description: "Direct flight to waypoint"
  - name: orbit
    id: 2
    description: "Circle around point"
  - name: approach
    id: 3
    description: "Slow approach to waypoint"
  - name: evade
    id: 4
    description: "Obstacle avoidance maneuver"
  - name: return
    id: 5
    description: "Return to safe point"
